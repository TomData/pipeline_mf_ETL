"""Walk-forward validation aggregation and report helpers."""

from __future__ import annotations

from dataclasses import dataclass
from datetime import datetime, timezone
import json
import os
from pathlib import Path
from typing import Any
from uuid import uuid4

import numpy as np
import polars as pl


@dataclass(frozen=True, slots=True)
class WalkForwardCollected:
    """Collected split-level and aggregate outputs for walk-forward runs."""

    split_runs_df: pl.DataFrame
    model_summary_long_df: pl.DataFrame
    model_summary_wide_df: pl.DataFrame
    comparison_summary_df: pl.DataFrame
    state_metrics_long_df: pl.DataFrame
    aggregate_summary: dict[str, Any]


@dataclass(frozen=True, slots=True)
class WalkForwardReportPaths:
    """Artifact paths generated by walk-forward reporting."""

    manifest_path: Path
    split_runs_path: Path
    model_summary_long_path: Path
    model_summary_wide_path: Path
    comparison_summary_path: Path
    aggregate_summary_path: Path
    state_metrics_long_path: Path
    full_report_path: Path


def _atomic_temp_path(target_path: Path) -> Path:
    return target_path.parent / f".{target_path.name}.{uuid4().hex}.tmp"


def _write_json_atomically(payload: dict[str, Any], output_path: Path) -> Path:
    output_path.parent.mkdir(parents=True, exist_ok=True)
    temp_path = _atomic_temp_path(output_path)
    try:
        temp_path.write_text(json.dumps(payload, indent=2, sort_keys=True, default=str) + "\n", encoding="utf-8")
        os.replace(temp_path, output_path)
    finally:
        if temp_path.exists():
            temp_path.unlink()
    return output_path


def _write_csv_atomically(df: pl.DataFrame, output_path: Path) -> Path:
    output_path.parent.mkdir(parents=True, exist_ok=True)
    temp_path = _atomic_temp_path(output_path)
    try:
        df.write_csv(temp_path)
        os.replace(temp_path, output_path)
    finally:
        if temp_path.exists():
            temp_path.unlink()
    return output_path


def _write_markdown_atomically(text: str, output_path: Path) -> Path:
    output_path.parent.mkdir(parents=True, exist_ok=True)
    temp_path = _atomic_temp_path(output_path)
    try:
        temp_path.write_text(text, encoding="utf-8")
        os.replace(temp_path, output_path)
    finally:
        if temp_path.exists():
            temp_path.unlink()
    return output_path


def _safe_float(value: Any) -> float | None:
    if value is None:
        return None
    try:
        out = float(value)
    except (TypeError, ValueError):
        return None
    return out if np.isfinite(out) else None


def _numeric_nan_count(df: pl.DataFrame) -> int:
    if df.height == 0:
        return 0
    numeric_cols = [column for column, dtype in df.schema.items() if dtype.is_numeric()]
    if not numeric_cols:
        return 0
    total = 0
    for column in numeric_cols:
        total += int(df.select(pl.col(column).cast(pl.Float64, strict=False).is_nan().fill_null(False).sum()).item())
    return total


def _load_json(path: Path) -> dict[str, Any]:
    return json.loads(path.read_text(encoding="utf-8"))


def _validation_nan_warning_total(run_dir: Path) -> int:
    state_scorecard = pl.read_csv(run_dir / "state_scorecard.csv")
    pairwise = pl.read_csv(run_dir / "bootstrap_pairwise_diff.csv")
    transition = pl.read_csv(run_dir / "transition_event_summary.csv")
    stability = pl.read_csv(run_dir / "state_stability_summary.csv")
    return _numeric_nan_count(state_scorecard) + _numeric_nan_count(pairwise) + _numeric_nan_count(transition) + _numeric_nan_count(stability)


def _extract_validation_metrics(
    *,
    train_end: str,
    model: str,
    validation_run_dir: Path,
) -> dict[str, Any]:
    run_summary = _load_json(validation_run_dir / "run_summary.json")
    scorecard = _load_json(validation_run_dir / "validation_scorecard.json")

    transition_summary = pl.read_csv(validation_run_dir / "transition_event_summary.csv")
    state_scorecard = pl.read_csv(validation_run_dir / "state_scorecard.csv")
    bootstrap_pairwise = pl.read_csv(validation_run_dir / "bootstrap_pairwise_diff.csv")

    row = {
        "train_end": train_end,
        "model": model,
        "validation_run_dir": str(validation_run_dir),
        "rows": int(run_summary.get("rows", 0)),
        "n_states": int(run_summary.get("state_count", 0)),
        "validation_grade": scorecard.get("validation_grade"),
        "forward_separation_score": _safe_float(scorecard.get("forward_separation_score")),
        "pairwise_diff_significant_share": _safe_float(scorecard.get("pairwise_diff_significant_share")),
        "avg_ci_width_fwd_ret_10": _safe_float(scorecard.get("avg_ci_width_fwd_ret_10")),
        "avg_state_sign_consistency": _safe_float(scorecard.get("avg_state_sign_consistency")),
        "avg_state_ret_cv": _safe_float(scorecard.get("avg_state_ret_cv")),
        "nan_warning_total": int(_validation_nan_warning_total(validation_run_dir)),
        "date_min": scorecard.get("date_min"),
        "date_max": scorecard.get("date_max"),
        "transition_summary_rows": int(transition_summary.height),
        "state_scorecard_rows": int(state_scorecard.height),
        "bootstrap_pairwise_rows": int(bootstrap_pairwise.height),
    }
    return row


def _extract_comparison_metrics(
    *,
    train_end: str,
    comparison_name: str,
    comparison_dir: Path,
) -> dict[str, Any]:
    payload = _load_json(comparison_dir / "validation_compare_summary.json")
    metric_diffs = payload.get("metric_diffs", [])
    metric_map: dict[str, dict[str, Any]] = {}
    for row in metric_diffs:
        metric = row.get("metric")
        if isinstance(metric, str):
            metric_map[metric] = row

    return {
        "train_end": train_end,
        "comparison": comparison_name,
        "comparison_dir": str(comparison_dir),
        "run_a_validation_grade": payload.get("run_a_validation_grade"),
        "run_b_validation_grade": payload.get("run_b_validation_grade"),
        "delta_forward_separation_score": _safe_float(metric_map.get("forward_separation_score", {}).get("delta_b_minus_a")),
        "delta_avg_ci_width_fwd_ret_10": _safe_float(metric_map.get("avg_ci_width_fwd_ret_10", {}).get("delta_b_minus_a")),
        "delta_avg_state_sign_consistency": _safe_float(metric_map.get("avg_state_sign_consistency", {}).get("delta_b_minus_a")),
        "delta_avg_state_ret_cv": _safe_float(metric_map.get("avg_state_ret_cv", {}).get("delta_b_minus_a")),
        "delta_pairwise_diff_significant_share": _safe_float(metric_map.get("pairwise_diff_significant_share", {}).get("delta_b_minus_a")),
    }


def _collect_state_metrics(
    *,
    train_end: str,
    model: str,
    validation_run_dir: Path,
) -> pl.DataFrame:
    state_scorecard = pl.read_csv(validation_run_dir / "state_scorecard.csv")
    if state_scorecard.height == 0:
        return pl.DataFrame(
            {
                "train_end": [],
                "model": [],
                "state_id": [],
            }
        )
    return state_scorecard.with_columns(
        [
            pl.lit(train_end).alias("train_end"),
            pl.lit(model).alias("model"),
            pl.lit(str(validation_run_dir)).alias("validation_run_dir"),
        ]
    )


def _build_wide_summary(model_long: pl.DataFrame) -> pl.DataFrame:
    if model_long.height == 0:
        return pl.DataFrame(schema={"train_end": pl.String})

    value_cols = [
        "rows",
        "n_states",
        "validation_grade",
        "forward_separation_score",
        "pairwise_diff_significant_share",
        "avg_ci_width_fwd_ret_10",
        "avg_state_sign_consistency",
        "avg_state_ret_cv",
        "nan_warning_total",
        "transition_summary_rows",
    ]

    by_model: list[pl.DataFrame] = []
    for model in ("HMM", "Flow", "Cluster"):
        subset = model_long.filter(pl.col("model") == model).select(["train_end", *value_cols])
        renamed = subset.rename({column: f"{model.lower()}_{column}" for column in value_cols})
        by_model.append(renamed)

    wide = by_model[0]
    for frame in by_model[1:]:
        wide = wide.join(frame, on="train_end", how="outer_coalesce")
    return wide.sort("train_end")


def _grade_counts(values: list[str | None]) -> dict[str, int]:
    out: dict[str, int] = {}
    for value in values:
        label = value if value is not None else "NULL"
        out[label] = out.get(label, 0) + 1
    return out


def _model_aggregate(model_long: pl.DataFrame, model: str) -> dict[str, Any]:
    subset = model_long.filter(pl.col("model") == model)
    if subset.height == 0:
        return {
            "splits_completed": 0,
            "total_rows_sum": 0,
        }

    rows = subset["rows"].cast(pl.Float64, strict=False).to_numpy()
    sep = subset["forward_separation_score"].cast(pl.Float64, strict=False).to_numpy()
    ci = subset["avg_ci_width_fwd_ret_10"].cast(pl.Float64, strict=False).to_numpy()
    sign = subset["avg_state_sign_consistency"].cast(pl.Float64, strict=False).to_numpy()
    ret_cv = subset["avg_state_ret_cv"].cast(pl.Float64, strict=False).to_numpy()
    pair = subset["pairwise_diff_significant_share"].cast(pl.Float64, strict=False).to_numpy()
    transitions = subset["transition_summary_rows"].cast(pl.Float64, strict=False).to_numpy()

    finite_sep = sep[np.isfinite(sep)]
    finite_ci = ci[np.isfinite(ci)]
    finite_sign = sign[np.isfinite(sign)]
    finite_ret_cv = ret_cv[np.isfinite(ret_cv)]
    finite_pair = pair[np.isfinite(pair)]
    finite_transitions = transitions[np.isfinite(transitions)]

    finite_rows = rows[np.isfinite(rows)]

    def weighted_mean(values: np.ndarray) -> float | None:
        if values.size == 0 or rows.size == 0:
            return None
        mask = np.isfinite(values) & np.isfinite(rows)
        if not np.any(mask):
            return None
        weighted_rows = rows[mask]
        weighted_values = values[mask]
        weight_sum = float(np.sum(weighted_rows))
        if weight_sum <= 0:
            return None
        return _safe_float(float(np.sum(weighted_values * weighted_rows) / weight_sum))

    score_map = {"A": 3, "B": 2, "C": 1}
    grades = subset["validation_grade"].cast(pl.String, strict=False).to_list()
    grade_scores = [score_map[g] for g in grades if g in score_map]

    return {
        "splits_completed": int(subset.height),
        "total_rows_sum": int(np.sum(finite_rows)) if finite_rows.size > 0 else 0,
        "rows_median": _safe_float(float(np.median(finite_rows))) if finite_rows.size > 0 else None,
        "forward_separation_score_mean": _safe_float(float(np.mean(finite_sep))) if finite_sep.size > 0 else None,
        "forward_separation_score_median": _safe_float(float(np.median(finite_sep))) if finite_sep.size > 0 else None,
        "forward_separation_score_std": _safe_float(float(np.std(finite_sep, ddof=0))) if finite_sep.size > 0 else None,
        "avg_ci_width_fwd_ret_10_mean": _safe_float(float(np.mean(finite_ci))) if finite_ci.size > 0 else None,
        "avg_ci_width_fwd_ret_10_median": _safe_float(float(np.median(finite_ci))) if finite_ci.size > 0 else None,
        "avg_state_sign_consistency_mean": _safe_float(float(np.mean(finite_sign))) if finite_sign.size > 0 else None,
        "avg_state_sign_consistency_median": _safe_float(float(np.median(finite_sign))) if finite_sign.size > 0 else None,
        "avg_state_ret_cv_mean": _safe_float(float(np.mean(finite_ret_cv))) if finite_ret_cv.size > 0 else None,
        "avg_state_ret_cv_median": _safe_float(float(np.median(finite_ret_cv))) if finite_ret_cv.size > 0 else None,
        "avg_state_ret_cv_p90": _safe_float(float(np.quantile(finite_ret_cv, 0.90))) if finite_ret_cv.size > 0 else None,
        "pairwise_diff_significant_share_mean": _safe_float(float(np.mean(finite_pair))) if finite_pair.size > 0 else None,
        "transition_summary_rows_mean": _safe_float(float(np.mean(finite_transitions))) if finite_transitions.size > 0 else None,
        "validation_grade_counts": _grade_counts(grades),
        "validation_grade_score_mean_heuristic": _safe_float(float(np.mean(grade_scores))) if grade_scores else None,
        "nan_warning_total_sum": int(subset["nan_warning_total"].sum()),
        "weighted_mean_forward_separation": weighted_mean(sep),
        "weighted_mean_avg_ci_width": weighted_mean(ci),
        "weighted_mean_sign_consistency": weighted_mean(sign),
    }


def _win_counts(model_long: pl.DataFrame) -> dict[str, dict[str, int]]:
    out = {
        "highest_separation": {"HMM": 0, "Flow": 0, "Cluster": 0},
        "lowest_ci_width": {"HMM": 0, "Flow": 0, "Cluster": 0},
        "best_sign_consistency": {"HMM": 0, "Flow": 0, "Cluster": 0},
    }

    if model_long.height == 0:
        return out

    split_values = model_long.select("train_end").to_series().unique().to_list()
    for split in split_values:
        split_df = model_long.filter(pl.col("train_end") == split)
        if split_df.height < 2:
            continue

        sep_df = split_df.filter(pl.col("forward_separation_score").is_not_null())
        if sep_df.height > 0:
            winner = sep_df.sort("forward_separation_score", descending=True)["model"][0]
            out["highest_separation"][winner] += 1

        ci_df = split_df.filter(pl.col("avg_ci_width_fwd_ret_10").is_not_null())
        if ci_df.height > 0:
            winner = ci_df.sort("avg_ci_width_fwd_ret_10", descending=False)["model"][0]
            out["lowest_ci_width"][winner] += 1

        sign_df = split_df.filter(pl.col("avg_state_sign_consistency").is_not_null())
        if sign_df.height > 0:
            winner = sign_df.sort("avg_state_sign_consistency", descending=True)["model"][0]
            out["best_sign_consistency"][winner] += 1

    return out


def collect_walkforward_outputs(
    split_records: list[dict[str, Any]],
) -> WalkForwardCollected:
    """Collect split/model metrics and aggregate walk-forward outputs."""

    split_runs_df = pl.DataFrame(split_records) if split_records else pl.DataFrame(schema={"train_end": pl.String})

    success_records = [record for record in split_records if record.get("status") == "SUCCESS"]

    model_rows: list[dict[str, Any]] = []
    comparison_rows: list[dict[str, Any]] = []
    state_frames: list[pl.DataFrame] = []

    for record in success_records:
        train_end = str(record.get("train_end"))
        val_hmm_dir = Path(str(record.get("val_hmm_dir")))
        val_flow_dir = Path(str(record.get("val_flow_dir")))
        val_cluster_dir = Path(str(record.get("val_cluster_dir")))

        model_rows.append(_extract_validation_metrics(train_end=train_end, model="HMM", validation_run_dir=val_hmm_dir))
        model_rows.append(_extract_validation_metrics(train_end=train_end, model="Flow", validation_run_dir=val_flow_dir))
        model_rows.append(
            _extract_validation_metrics(train_end=train_end, model="Cluster", validation_run_dir=val_cluster_dir)
        )

        state_frames.append(_collect_state_metrics(train_end=train_end, model="HMM", validation_run_dir=val_hmm_dir))
        state_frames.append(_collect_state_metrics(train_end=train_end, model="Flow", validation_run_dir=val_flow_dir))
        state_frames.append(
            _collect_state_metrics(train_end=train_end, model="Cluster", validation_run_dir=val_cluster_dir)
        )

        cmp_hf_dir = Path(str(record.get("cmp_hmm_flow_dir")))
        cmp_hc_dir = Path(str(record.get("cmp_hmm_cluster_dir")))
        comparison_rows.append(
            _extract_comparison_metrics(
                train_end=train_end,
                comparison_name="HMM_vs_Flow",
                comparison_dir=cmp_hf_dir,
            )
        )
        comparison_rows.append(
            _extract_comparison_metrics(
                train_end=train_end,
                comparison_name="HMM_vs_Cluster",
                comparison_dir=cmp_hc_dir,
            )
        )

    model_summary_long_df = (
        pl.DataFrame(model_rows).sort(["train_end", "model"]) if model_rows else pl.DataFrame(schema={"train_end": pl.String, "model": pl.String})
    )
    model_summary_wide_df = _build_wide_summary(model_summary_long_df)
    comparison_summary_df = (
        pl.DataFrame(comparison_rows).sort(["train_end", "comparison"]) if comparison_rows else pl.DataFrame(schema={"train_end": pl.String, "comparison": pl.String})
    )
    state_metrics_long_df = pl.concat(state_frames, how="diagonal_relaxed") if state_frames else pl.DataFrame(schema={"train_end": pl.String, "model": pl.String})

    aggregate_by_model = {
        model: _model_aggregate(model_summary_long_df, model) for model in ("HMM", "Flow", "Cluster")
    }

    aggregate_summary = {
        "generated_ts": datetime.now(timezone.utc).isoformat(),
        "splits_total": len(split_records),
        "splits_successful": len(success_records),
        "splits_failed": int(sum(1 for record in split_records if record.get("status") == "FAILED")),
        "failed_splits": [
            {
                "train_end": record.get("train_end"),
                "error": record.get("error"),
            }
            for record in split_records
            if record.get("status") == "FAILED"
        ],
        "aggregate_by_model": aggregate_by_model,
        "wins_by_metric": _win_counts(model_summary_long_df),
    }

    return WalkForwardCollected(
        split_runs_df=split_runs_df,
        model_summary_long_df=model_summary_long_df,
        model_summary_wide_df=model_summary_wide_df,
        comparison_summary_df=comparison_summary_df,
        state_metrics_long_df=state_metrics_long_df,
        aggregate_summary=aggregate_summary,
    )


def _render_walkforward_report(
    *,
    wf_run_id: str,
    dataset_path: Path,
    train_end_list: list[str],
    wf_manifest: dict[str, Any],
    collected: WalkForwardCollected,
) -> str:
    long_df = collected.model_summary_long_df

    lines: list[str] = []
    lines.append(f"# Walk-Forward OOS Validation Report ({wf_run_id})")
    lines.append("")
    lines.append("## Configuration")
    lines.append(f"- dataset: `{dataset_path}`")
    lines.append(f"- train_end_list: `{','.join(train_end_list)}`")
    lines.append(f"- hmm_components: `{wf_manifest.get('config', {}).get('hmm_components')}`")
    lines.append(f"- cluster_method: `{wf_manifest.get('config', {}).get('cluster_method')}`")
    lines.append(f"- cluster_k: `{wf_manifest.get('config', {}).get('cluster_k')}`")
    lines.append(f"- scaling_scope: `{wf_manifest.get('config', {}).get('scaling_scope')}`")
    lines.append(
        "- validation: "
        f"bootstrap_n={wf_manifest.get('config', {}).get('bootstrap_n')}, "
        f"bootstrap_mode={wf_manifest.get('config', {}).get('bootstrap_mode')}, "
        f"block_length={wf_manifest.get('config', {}).get('block_length')}, "
        f"event_window_pre={wf_manifest.get('config', {}).get('event_window_pre')}, "
        f"event_window_post={wf_manifest.get('config', {}).get('event_window_post')}, "
        f"min_events_per_transition={wf_manifest.get('config', {}).get('min_events_per_transition')}"
    )
    lines.append("")

    lines.append("## Split Status")
    lines.append("| train_end | status | error |")
    lines.append("|---|---|---|")
    for record in wf_manifest.get("splits", []):
        lines.append(
            f"| {record.get('train_end')} | {record.get('status')} | {str(record.get('error') or '').replace('|', '/')} |"
        )
    lines.append("")

    lines.append("## Split x Model Metrics")
    lines.append("| train_end | model | rows | states | grade | separation | avg_ci_width | sign_consistency | ret_cv | pairwise_sig_share | transition_rows | nan_warnings |")
    lines.append("|---|---|---:|---:|---|---:|---:|---:|---:|---:|---:|---:|")
    for row in long_df.to_dicts():
        lines.append(
            "| "
            f"{row.get('train_end')} | {row.get('model')} | {row.get('rows')} | {row.get('n_states')} | {row.get('validation_grade')} | "
            f"{row.get('forward_separation_score')} | {row.get('avg_ci_width_fwd_ret_10')} | {row.get('avg_state_sign_consistency')} | "
            f"{row.get('avg_state_ret_cv')} | {row.get('pairwise_diff_significant_share')} | {row.get('transition_summary_rows')} | {row.get('nan_warning_total')} |"
        )
    lines.append("")

    lines.append("## Aggregate Summary")
    for model, payload in collected.aggregate_summary.get("aggregate_by_model", {}).items():
        lines.append(f"### {model}")
        lines.append(f"- splits_completed: {payload.get('splits_completed')}")
        lines.append(f"- total_rows_sum: {payload.get('total_rows_sum')}")
        lines.append(f"- forward_separation_score_mean: {payload.get('forward_separation_score_mean')}")
        lines.append(f"- avg_ci_width_fwd_ret_10_mean: {payload.get('avg_ci_width_fwd_ret_10_mean')}")
        lines.append(f"- avg_state_sign_consistency_mean: {payload.get('avg_state_sign_consistency_mean')}")
        lines.append(f"- avg_state_ret_cv_mean: {payload.get('avg_state_ret_cv_mean')}")
        lines.append(f"- pairwise_diff_significant_share_mean: {payload.get('pairwise_diff_significant_share_mean')}")
    lines.append("")

    wins = collected.aggregate_summary.get("wins_by_metric", {})
    lines.append("## Wins By Metric")
    lines.append(f"- highest_separation: {wins.get('highest_separation')}")
    lines.append(f"- lowest_ci_width: {wins.get('lowest_ci_width')}")
    lines.append(f"- best_sign_consistency: {wins.get('best_sign_consistency')}")
    lines.append("")

    lines.append("## Interpretation")
    lines.append("- Walk-forward summary consolidates HMM/Flow/Cluster metrics across successful OOS splits.")
    lines.append("- Weighted means use split-level row counts to reduce small-split bias.")
    lines.append("- Grade aggregation is reported as counts; optional grade-score average is heuristic.")
    lines.append("- Failed splits, if any, are listed explicitly in split status and aggregate summary.")

    return "\n".join(lines) + "\n"


def write_walkforward_outputs(
    *,
    output_dir: Path,
    wf_manifest: dict[str, Any],
    dataset_path: Path,
    train_end_list: list[str],
    collected: WalkForwardCollected,
) -> WalkForwardReportPaths:
    """Write all walk-forward summary artifacts and markdown report."""

    output_dir.mkdir(parents=True, exist_ok=True)
    manifest_path = output_dir / "wf_manifest.json"
    split_runs_path = output_dir / "wf_split_runs.csv"
    model_summary_long_path = output_dir / "wf_model_summary_long.csv"
    model_summary_wide_path = output_dir / "wf_model_summary_wide.csv"
    comparison_summary_path = output_dir / "wf_comparison_summary.csv"
    aggregate_summary_path = output_dir / "wf_aggregate_summary.json"
    state_metrics_long_path = output_dir / "wf_state_metrics_long.csv"
    full_report_path = output_dir / "wf_full_report.md"

    _write_json_atomically(wf_manifest, manifest_path)
    _write_csv_atomically(collected.split_runs_df, split_runs_path)
    _write_csv_atomically(collected.model_summary_long_df, model_summary_long_path)
    _write_csv_atomically(collected.model_summary_wide_df, model_summary_wide_path)
    _write_csv_atomically(collected.comparison_summary_df, comparison_summary_path)
    _write_json_atomically(collected.aggregate_summary, aggregate_summary_path)
    _write_csv_atomically(collected.state_metrics_long_df, state_metrics_long_path)

    report_text = _render_walkforward_report(
        wf_run_id=str(wf_manifest.get("wf_run_id")),
        dataset_path=dataset_path,
        train_end_list=train_end_list,
        wf_manifest=wf_manifest,
        collected=collected,
    )
    _write_markdown_atomically(report_text, full_report_path)

    return WalkForwardReportPaths(
        manifest_path=manifest_path,
        split_runs_path=split_runs_path,
        model_summary_long_path=model_summary_long_path,
        model_summary_wide_path=model_summary_wide_path,
        comparison_summary_path=comparison_summary_path,
        aggregate_summary_path=aggregate_summary_path,
        state_metrics_long_path=state_metrics_long_path,
        full_report_path=full_report_path,
    )
